{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get-data-from-cdcs-test-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sandbox for developing documentation resources (example notebooks and demo videos) for the SciServer AMBench project.\n",
    "\n",
    "This notebook includes some methods for querying and obtaining results from the ambench.nist.gov website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: prepare the notebook to do what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import required packages\n",
    "\n",
    "First, we need to import packages that we will use to do the processing. Each import statement includes a comment describing what that package does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T13:09:02.706083Z",
     "iopub.status.busy": "2022-04-26T13:09:02.704915Z",
     "iopub.status.idle": "2022-04-26T13:09:17.372182Z",
     "shell.execute_reply": "2022-04-26T13:09:17.369378Z",
     "shell.execute_reply.started": "2022-04-26T13:09:02.706006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/lmhale99/pycdcs\n",
    "# print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T13:09:21.419721Z",
     "iopub.status.busy": "2022-04-26T13:09:21.418952Z",
     "iopub.status.idle": "2022-04-26T13:09:22.808381Z",
     "shell.execute_reply": "2022-04-26T13:09:22.806395Z",
     "shell.execute_reply.started": "2022-04-26T13:09:21.419669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages imported!\n"
     ]
    }
   ],
   "source": [
    "### Data processing packages: \n",
    "import pandas # data processing\n",
    "import math # math operations\n",
    "import numpy as np # math operations\n",
    "\n",
    "\n",
    "### JORDAN NOTE TO SELF: Call it \"AMBench repository\" instead of / in addition to \"CDCS\"\n",
    "### JORDAN NOTE TO SELF: Add a description here about what CDCS is and why you want to use it\n",
    "### Packages to download data from the Configurable Data Curation System (CDCS):\n",
    "##### CDCS website: https://www.nist.gov/itl/ssd/information-systems-group/configurable-data-curation-system-cdcs\n",
    "from cdcs import CDCS         # query CDCS data\n",
    "import lxml.etree as et       # parse the XML file returned by CDCS to find image download locations\n",
    "import requests               # download images\n",
    "from urllib import request    # download images\n",
    "import os                     # save and load image files\n",
    "\n",
    "### Packages to do image processing once we have the images downloaded:\n",
    "##### These are from the scikit-image package, but we only import the parts of the package that we need \n",
    "##### scikit-image website: https://scikit-image.org/\n",
    "##### JORDAN NOTE TO SELF: Be more specific about what kind of processing each sub-package does\n",
    "##### JORDAN NOTE TO SELF: Try it as \"from skimage import io, filters, segmentation, morphology, measure\"\n",
    "from skimage import io           # read/write images\n",
    "from skimage import filters      # process images\n",
    "from skimage import segmentation # process images\n",
    "from skimage import morphology   # process images\n",
    "from skimage import measure      # process images\n",
    "\n",
    "### Packages to plot results:\n",
    "import matplotlib.pyplot as plt     # make plots\n",
    "import matplotlib.image as mpimg    # make plots related to images, including displaying images in colorscale (\"False Color\")\n",
    "\n",
    "\n",
    "print('Packages imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions\n",
    "\n",
    "For some complex operations that we need to do repeatedly, it's easier to move the operations into a function call. This slows down the code slightly, but makes it much easier to read and to follow the logic of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "#### xml_url_find:\n",
    "# Parameters:\n",
    "# - XML file for one of the ambench.nist.gov datasets in string format\n",
    "# - search phrase contained in the file name\n",
    "# - file type\n",
    "\n",
    "# Returns:\n",
    "# - a list containing (0) name of a file, (1) the download url for the file, (2) laser track number, (3) case \n",
    "def xml_url_find(xml,searchphrase,mtype):\n",
    "    print('\\tin xml_url_find...')\n",
    "    root=et.fromstring(xml)\n",
    "    print('\\t\\t{0:}'.format(root))\n",
    "    caseid=root.find('.//TraceID')[0].tag[5]\n",
    "    track=root.find('.//TrackNumber')\n",
    "    for element in root.iter('downloadURL'):\n",
    "        u=request.urlopen(element.text)\n",
    "        if searchphrase in u.info().get_filename() and mtype in u.info().get_content_type(): #looking for searchphrase in\n",
    "            #filename and mtype in file type\n",
    "            name=u.info().get_filename()\n",
    "            url=element.text\n",
    "            return [name,url,track.text,caseid]\n",
    "\n",
    "#Given an xml string like the one returned by CDCS_xml, will return a dataframe of download links for the files on the AMBench\n",
    "#page (includes name, description, comments, etc.)\n",
    "def CDCS_downloads(xml):\n",
    "    root=et.fromstring(xml)\n",
    "    result=[]\n",
    "    for element in root.iter('downloadURL'):\n",
    "        entry={}\n",
    "        for e in element.getparent():\n",
    "            entry[e.tag]=e.text\n",
    "        result.append(entry)\n",
    "    df=py.DataFrame.from_dict(result,orient='columns')\n",
    "    return df        \n",
    "\n",
    "\n",
    "#### draw_box_opt\n",
    "# This method (1) crops the melt pool images (to reduce runtime), (2) applies the felzenszwalb segmentation algorithm to the image, (3) removes smaller regions (eliminate some background noise), (4) acquires a table of center coordinates for the remaining regions, (5) measures the distances from centers to the center of the image, (6) finds the index number of the closest region, (7) returns information about the closes region\n",
    "\n",
    "# Parameters:\n",
    "# - image to be analyzed\n",
    "\n",
    "# Returns:\n",
    "# - list of RegionProperties related to the melt pool region\n",
    "def draw_box_opt(image):\n",
    "    cropim=image[900:1800,800:3400] #cropping so that there's less pixels to cover - incredibly slow if left at original size\n",
    "    segments=segmentation.felzenszwalb(cropim,scale=270,sigma=0.8,min_size=1000)\n",
    "    isolateim=morphology.remove_small_objects(segments,100000)\n",
    "    center=measure.regionprops_table(isolateim,properties=['centroid']) #table of information about the center points of regions\n",
    "    distances=[]\n",
    "    for n in range(len(center['centroid-0'])):\n",
    "        distances.append(math.dist([center['centroid-0'][n],center['centroid-1'][n]],[450,1300]))\n",
    "    index=distances.index(min(distances)) #Looking for the region closest to the center of the image - should be the melt pool\n",
    "    object_features=measure.regionprops(isolateim)\n",
    "    return object_features[index] \n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import records from CDCS\n",
    "\n",
    "The first step is to download the data we need from NIST. We search the Configurable Data Curation System (CDCS) for a keyword that will find the data we are looking for. In this example, the keyword is \"MP\" (for \"melt pool\"), which will return datasets associated with the Melt Pool Challenge.\n",
    "\n",
    "Another option is to use the keyword \"GS\", which will retrieve scanning electron microscope (SEM) images of melt pools taken as part of the grain size (GS) AM-Bench Challenge.\n",
    "\n",
    "The notebook returns all public datasets with your selected keyword. You can see information about each returned dataset by running the code block with the `describe_datasets = True`.\n",
    "\n",
    "JORDAN NOTE TO SELF: Replace \"CDCS\" with \"AMBench repository\"\n",
    "\n",
    "JORDAN NOTE TO SELF: Look at Lyle's data management diagram to make sure we are using the same terms\n",
    "\n",
    " - uses the CDCS REST API client to search for datasets related to challenges within the ambench.nist.gov instance with these keywords\n",
    "    - MP: search for Melt Pool challenges\n",
    "    - GS: retrieve scanning electron microscope (SEM) images of melt pools taken as part of the grain size (GS) AM-Bench challenge \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keyword='MP'                # keyword to search for: MP = melt pool, GS = grain size\n",
    "describe_datasets = True    # show name and basic metadata for each dataset\n",
    "show_dataset_xml = False    # parse xml to show full metadata for each dataset\n",
    "\n",
    "\n",
    "print('querying CDCS for keyword {0:}...'.format(keyword))\n",
    "curator = CDCS('https://ambench.nist.gov/', username='') # query CDCS, accessing anonymously\n",
    "datasets_df = curator.query(template='AM-Bench-2018',keyword=keyword) # searching for results with the keyword given; returns a pandas dataframe with a row for each dataset\n",
    "\n",
    "### convert all date fields to pandas date format, then use the dataset ID as the index of the table and sort by ID\n",
    "for thiscol in ['creation_date', 'last_modification_date', 'last_change_date']:\n",
    "    datasets_df.loc[:, thiscol] = pandas.to_datetime(datasets_df[thiscol], errors='coerce')\n",
    "# datasets_df = datasets_df.set_index('id')\n",
    "# datasets_df = datasets_df.sort_index()\n",
    "\n",
    "print('Found {0:,.0f} datasets matching keyword {1:}!'.format(len(datasets_df), keyword))\n",
    "print('\\n')\n",
    "\n",
    "if (describe_datasets):\n",
    "    for ix, thisrow in datasets_df.iterrows():\n",
    "        print('Dataset id = {0:}'.format(thisrow['id']))\n",
    "        print('\\tTitle: {0:}'.format(thisrow['title']))\n",
    "        print('\\tCreated: {0:}'.format(thisrow['creation_date'].strftime('%Y-%m-%d')))\n",
    "#         print('\\tLast modified: {0:}'.format(thisrow['last_modification_date'].strftime('%Y-%m-%d')))\n",
    "#         print('\\tLast changed: {0:}'.format(thisrow['last_change_date'].strftime('%Y-%m-%d')))\n",
    "        if (show_dataset_xml):\n",
    "            show_xml(thisrow['xml_content'])\n",
    "        print('\\n')\n",
    "\n",
    "datasets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and process images from those datasets\n",
    "\n",
    "In addition to the metadata for each dataset, CDCS returns the dataset itself - but in XML format, which is not intended to be human readable. The dataset XML is given in the `xml_content` column.\n",
    "\n",
    "This code cell uses the function `xml_url_find`, created in the setup step, to search for the names of the TIFF image files to associated with these datasets. For the purposes of this demo notebook, consider that function as a black box; if you want to learn how it works, see the function documentation in the setup step above.\n",
    "\n",
    "Each dataset includes several images (TIFF files), each associated with a specific track and case number. The code cell below returns these track and case numbers, as well as the URL where the associated image can be downloaded.\n",
    "\n",
    "The code cell then downloads the image from that URL, one at a time, and runs an image processing algorithm to measure the width and depth of the melt pool in the image. Set `show_image = True` to see each image as it is downloaded from the NIST server (note that the images themselves are not saved, only the download URL and the measured parameters.\n",
    "\n",
    "All the image processing happens in the `draw_box_opt` function, created in the setup step.\n",
    "\n",
    "The code cell outputs a list of measured melt pool parameters for each dataset.\n",
    "\n",
    "JORDAN NOTE TO SELF: Better distinguish between metadata and data\n",
    "\n",
    "\n",
    "### Previous text to integrate into the above:\n",
    "\n",
    "General approach to image analysis:\n",
    "- using scikit-image to perform image analysis and matplotlib to display images\n",
    "- uses the felzenszwalb image segmentation algorithm to help obtain widths and depths of the melt pools\n",
    "- displays plots of width and depth\n",
    "- stores measurements in a csv file\n",
    "\n",
    "#### draw_box_opt\n",
    "This method (1) crops the melt pool images (to reduce runtime), (2) applies the felzenszwalb segmentation algorithm to the image, (3) removes smaller regions (eliminate some background noise), (4) acquires a table of center coordinates for the remaining regions, (5) measures the distances from centers to the center of the image, (6) finds the index number of the closest region, (7) returns information about the closes region\n",
    "\n",
    "Parameters:\n",
    "- image to be analyzed\n",
    "\n",
    "Returns:\n",
    "- list of RegionProperties related to the melt pool region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchphrase='BF'     # used by the xml_url_find function; returns track, case, and image URL for each row in the dataset\n",
    "show_images = True   # set to True to see what the images look like as they are found\n",
    "\n",
    "print('finding...')\n",
    "datasets_df = datasets_df.assign(filename = np.nan, url = np.nan, track = np.nan, case = np.nan, melt_pool_width_microns = np.nan, melt_pool_depth_microns = np.nan)\n",
    "\n",
    "\n",
    "for ix, thisrow in datasets_df.iterrows():\n",
    "#    print(thisrow['xml_content'])\n",
    "    print(ix)\n",
    "    print('---------')\n",
    "    res=xml_url_find(thisrow['xml_content'],'BF','image/tiff') # giving xml_url_find each xml string and searching for\n",
    "    print('---------')\n",
    "\n",
    "    \n",
    "#     datasets_df.loc[ix, ['filename', 'url', 'track', 'case']] = res\n",
    "    \n",
    "#     print('Getting image for track {0:,.0f} case {1:}...'.format(thisrow['track'], thisrow['case']))\n",
    "    \n",
    "#     this_image = io.imread(thisrow['url'])\n",
    "    \n",
    "#     if (show_images):\n",
    "#         print('showing marked image...')\n",
    "#         fig, ax = plt.subplots(1,1)\n",
    "#         ax.imshow(this_image)\n",
    "    \n",
    "#     print('\\tmeasuring melt pool in image...')\n",
    "#     minr,minc,maxr,maxc = draw_box_opt(this_image).bbox #using the bbox drawn around the melt pool to find width and depth\n",
    "    \n",
    "#     datasets_df.loc[ix, 'melt_pool_width_microns'] = (maxc-minc)*(0.062)\n",
    "#     datasets_df.loc[ix, 'melt_pool_depth_microns'] = (maxr-minr)*(0.062)\n",
    "\n",
    "\n",
    "# datasets_df.loc[:, 'track'] = pandas.to_numeric(datasets_df['track'], downcast='integer', errors='coerce')\n",
    "# datasets_df = datasets_df[['title', 'track', 'case', 'melt_pool_width_microns', 'melt_pool_depth_microns', 'template', 'workspace',  'creation_date', 'last_modification_date', 'last_change_date', 'filename', 'url', 'xml_content', 'template_title', 'user_id']]\n",
    "# datasets_df = datasets_df.sort_values(by='track')\n",
    "\n",
    "\n",
    "# datasets_df[['filename', 'track', 'case']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-26T13:09:38.676364Z",
     "iopub.status.busy": "2022-04-26T13:09:38.675524Z",
     "iopub.status.idle": "2022-04-26T13:09:38.717478Z",
     "shell.execute_reply": "2022-04-26T13:09:38.715334Z",
     "shell.execute_reply.started": "2022-04-26T13:09:38.676304Z"
    }
   },
   "source": [
    "## 4. Make plots of melt pool sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of melt pool widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_df[['melt_pool_width_microns', 'track']].plot(y='melt_pool_width_microns',x='track',kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of melt pool depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_df[['melt_pool_depth_microns', 'track']].plot(y='melt_pool_depth_microns',x='track',kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_df.plot('melt_pool_depth_microns', 'melt_pool_depth_microns', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what the image looks like with the bbox marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# minr,minc,maxr,maxc=bboxes[4]\n",
    "# ax.imshow(result[4][900:1800,800:3400])\n",
    "# bx = (minc, maxc, maxc, minc, minc)\n",
    "# by = (minr, minr, maxr, maxr, minr)\n",
    "# ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# depths=[]\n",
    "# widths=[]\n",
    "# bboxes=[]\n",
    "# for im in dfres['image']:\n",
    "#     objf=draw_box_opt(im)\n",
    "#     bboxes.append(objf.bbox)\n",
    "#     minr,minc,maxr,maxc=objf.bbox #using the bbox drawn around the melt pool to find width and depth\n",
    "#     widths.append((maxc-minc)*(0.062)) #using the conversion given in the CDCS comment above the images to convert to mm (1 pixel=0.062 microns)\n",
    "#     depths.append((maxr-minr)*(0.062))\n",
    "# dat={'width':widths,'depth':depths,'track':trace,'case':case}\n",
    "# dfmp=pandas.DataFrame(dat)\n",
    "# dfmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig = plt.subplots(1,1)\n",
    "\n",
    "# for ix, thisrow in datasets_df.iterrows():\n",
    "    \n",
    "#     print('Getting image for track {0:,.0f} case {1:}...'.format(thisrow['track'], thisrow['case']))\n",
    "#     this_image = io.imread(thisrow['url'])\n",
    "\n",
    "#     print('\\tmeasuring melt pool in image...')\n",
    "#     minr,minc,maxr,maxc = draw_box_opt(this_image).bbox #using the bbox drawn around the melt pool to find width and depth\n",
    "\n",
    "#     datasets_df.loc[ix, 'melt_pool_width_microns'] = (maxc-minc)*(0.062)\n",
    "#     datasets_df.loc[ix, 'melt_pool_depth_microns'] = (maxr-minr)*(0.062)\n",
    "    \n",
    "#     print('\\tshowing marked image...')    \n",
    "#     fig, ax = plt.subplots(1,1)\n",
    "    \n",
    "#     bx = (minc, maxc, maxc, minc, minc)\n",
    "#     by = (minr, minr, maxr, maxr, minr)\n",
    "    \n",
    "#     ax.imshow(this_image[900:1800,800:3400])\n",
    "#     ax.plot(bx, by, '-b', linewidth=2.5)\n",
    "#     ax.set_title('Track {0:.0f} case {1:} (width = {2:.1f} µm, depth = {3:.1f} µm)'.format(thisrow['track'], thisrow['case'], datasets_df.loc[ix]['melt_pool_width_microns'], datasets_df.loc[ix]['melt_pool_depth_microns']))\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "#     #using the conversion given in the CDCS comment above the images to convert to mm (1 pixel=0.062 microns)\n",
    "# # objf\n",
    "# #datasets_df.head(2)\n",
    "# #plt.show()\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_xml(xml):\n",
    "#     print('xml tree:')\n",
    "#     print('---------')\n",
    "#     root = et.fromstring(xml)\n",
    "#     for i in range(0, len(root)):\n",
    "#         if (len(root[i]) <= 1):\n",
    "#             print('{0:.0f}. {1:}: {2:}'.format(i, root[i].tag, root[i].text))\n",
    "#         else:\n",
    "#             print('{0:.0f}. {1:} has {2:.0f} elements!'.format(i, root[i].tag, len(root[i])))\n",
    "#             for j in range(0, len(root[i])):\n",
    "#                 if (len(root[i][j]) <= 1):\n",
    "#                     print('\\t{0:.0f}.{1:.0f}. {2:}: {3:}'.format(i, j, root[i][j].tag, root[i][j].text))\n",
    "#                 else:\n",
    "#                     print('\\t{0:.0f}.{1:.0f}. {2:} has {3:.0f} elements!'.format(i, j, root[i][j].tag, len(root[i][j])))\n",
    "#                     for k in range(0, len(root[i][j])):\n",
    "#                         if (len(root[i][j][k]) <= 1):\n",
    "#                             print('\\t\\t{0:.0f}.{1:.0f}.{2:.0f}. {3:}: {4:}'.format(i, j, k, root[i][j][k].tag, root[i][j][k].text))\n",
    "#                         else:\n",
    "#                             print('\\t\\t\\t{0:.0f}.{1:.0f}.{2:.0f}. {3:} has {4:.0f} elements!'.format(i, j, k, root[i][j][k].tag, len(root[i][j][k])))\n",
    "#                             for l in range(0, len(root[i][j][k])):\n",
    "#                                 if (len(root[i][j][k][l]) <= 1):\n",
    "#                                     print('\\t\\t\\t{0:.0f}.{1:.0f}.{2:.0f}.{3:.0f}. {4:}: {5:}'.format(i, j, k, l, root[i][j][k][l].tag, root[i][j][k][l].text))\n",
    "#                                 else:\n",
    "#                                     for m in range(0, len(root[i][j][k][l])):\n",
    "#                                         if (len(root[i][j][k][l][m]) <= 1):\n",
    "#                                             print('\\t\\t\\t\\t{0:.0f}.{1:.0f}.{2:.0f}.{3:.0f}.{4:.0f}. {5:}: {6:}'.format(i, j, k, l, m, root[i][j][k][l][m].tag, root[i][j][k][l][m].text))\n",
    "#                                         else:\n",
    "#                                             print('\\t\\t\\t\\t{0:.0f}.{1:.0f}.{2:.0f}.{3:.0f}.{4:.0f}. {5:} has {6:.0f} elements!'.format(i, j, k, l, m, root[i][j][k][l][m].tag, len(root[i][j][k][l][m])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
